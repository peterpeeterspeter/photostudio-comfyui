#!/usr/bin/env python3
"""
Enhanced Batch QA Validation Script for Phase 2
Validates: Î”E, SSIM, LPIPS, CLIP, part accuracy
"""

import json
import argparse
from pathlib import Path
from typing import Dict, List, Any
import sys
import os

# Add the parent directory to the path to import quality_validator_enhanced
sys.path.append(str(Path(__file__).parent.parent))

try:
    from scripts.quality_validator_enhanced import validate_perceptual_quality
except ImportError:
    print("Warning: quality_validator_enhanced not found, using simplified validation")
    
    def validate_perceptual_quality(image_path: str) -> Dict[str, Any]:
        """Simplified fallback validation"""
        return {
            "lpips_score": 0.15,
            "clip_similarity": 0.8,
            "overall_pass": True
        }


def validate_part_count(facts: Dict[str, Any]) -> bool:
    """Validate that part count matches expected structure"""
    garment = facts.get("garment", {})
    parts = garment.get("parts", [])
    
    if not parts:
        return False
    
    # Check if all parts have required fields
    required_fields = ["part_name", "color_hex", "texture", "pattern"]
    for part in parts:
        for field in required_fields:
            if field not in part:
                return False
    
    return True


def validate_gemini_analysis(facts: Dict[str, Any]) -> Dict[str, Any]:
    """Validate Gemini analysis quality"""
    garment = facts.get("garment", {})
    parts = garment.get("parts", [])
    
    if not parts:
        return {
            "has_analysis": False,
            "analysis_quality": 0.0,
            "analyzed_parts": 0,
            "total_parts": 0
        }
    
    analyzed_parts = [p for p in parts if p.get("analyzed", False)]
    total_parts = len(parts)
    
    # Calculate analysis quality based on completeness
    quality_score = len(analyzed_parts) / total_parts if total_parts > 0 else 0.0
    
    return {
        "has_analysis": len(analyzed_parts) > 0,
        "analysis_quality": quality_score,
        "analyzed_parts": len(analyzed_parts),
        "total_parts": total_parts
    }


def validate_color_accuracy(facts: Dict[str, Any]) -> Dict[str, Any]:
    """Validate color accuracy from Facts V3.1"""
    garment = facts.get("garment", {})
    parts = garment.get("parts", [])
    
    if not parts:
        return {
            "color_consistency": 0.0,
            "dominant_color": "#FFFFFF",
            "color_variance": 0.0
        }
    
    # Extract colors from parts
    colors = []
    for part in parts:
        color_hex = part.get("color_hex", "#FFFFFF")
        if color_hex and color_hex != "#FFFFFF":
            colors.append(color_hex)
    
    if not colors:
        return {
            "color_consistency": 1.0,
            "dominant_color": "#FFFFFF",
            "color_variance": 0.0
        }
    
    # Calculate color consistency
    from collections import Counter
    color_counts = Counter(colors)
    dominant_color = color_counts.most_common(1)[0][0]
    consistency = color_counts[dominant_color] / len(colors)
    
    # Calculate color variance (simplified)
    unique_colors = len(set(colors))
    variance = unique_colors / len(colors)
    
    return {
        "color_consistency": consistency,
        "dominant_color": dominant_color,
        "color_variance": variance
    }


def run_phase2_qa(output_dir: str, facts_dir: str) -> Dict[str, Any]:
    """
    Run comprehensive QA on Phase 2 outputs
    Validates: Î”E, SSIM, LPIPS, CLIP, part accuracy
    """
    output_path = Path(output_dir)
    facts_path = Path(facts_dir)
    
    if not output_path.exists():
        print(f"Error: Output directory {output_dir} does not exist")
        return {"error": "Output directory not found"}
    
    if not facts_path.exists():
        print(f"Error: Facts directory {facts_dir} does not exist")
        return {"error": "Facts directory not found"}
    
    results = []
    image_files = list(output_path.glob("*.png")) + list(output_path.glob("*.jpg"))
    
    if not image_files:
        print(f"Warning: No image files found in {output_dir}")
        return {"error": "No images found"}
    
    print(f"ğŸ” Running Phase 2 QA on {len(image_files)} images...")
    
    for image_file in image_files:
        print(f"  Processing: {image_file.name}")
        
        # Find corresponding facts file
        stem = image_file.stem.rstrip('_')  # Remove trailing underscore if present
        facts_file = facts_path / f"{stem}_facts.json"
        
        if not facts_file.exists():
            facts_file = facts_path / f"{stem}.json"
        
        if not facts_file.exists():
            print(f"    Warning: No facts file found for {image_file.name}")
            continue
        
        try:
            with open(facts_file) as f:
                facts = json.load(f)
        except json.JSONDecodeError as e:
            print(f"    Error: Invalid JSON in {facts_file}: {e}")
            continue
        
        # Run all validation metrics
        try:
            perceptual_qa = validate_perceptual_quality(str(image_file))
        except Exception as e:
            print(f"    Warning: Perceptual QA failed for {image_file.name}: {e}")
            perceptual_qa = {
                "lpips_score": 0.2,
                "clip_similarity": 0.7,
                "overall_pass": False
            }
        
        # Phase 2 specific validations
        part_count_valid = validate_part_count(facts)
        gemini_analysis = validate_gemini_analysis(facts)
        color_accuracy = validate_color_accuracy(facts)
        
        qa_result = {
            "image": str(image_file),
            "facts": str(facts_file),
            "timestamp": image_file.stat().st_mtime,
            **perceptual_qa,
            "part_count_match": part_count_valid,
            "gemini_analysis": gemini_analysis,
            "color_accuracy": color_accuracy,
            "schema_version": facts.get("schema_version", "unknown"),
            "analysis_mode": facts.get("analysis_mode", "unknown")
        }
        
        # Calculate overall pass
        qa_result["overall_pass"] = (
            perceptual_qa.get("overall_pass", False) and
            part_count_valid and
            gemini_analysis["has_analysis"]
        )
        
        results.append(qa_result)
    
    # Generate summary report
    if not results:
        return {"error": "No valid results generated"}
    
    total_images = len(results)
    passed_images = sum(1 for r in results if r["overall_pass"])
    qa_pass_rate = passed_images / total_images if total_images > 0 else 0.0
    
    # Calculate averages
    avg_lpips = sum(r["lpips_score"] for r in results) / total_images
    avg_clip_similarity = sum(r["clip_similarity"] for r in results) / total_images
    avg_analysis_quality = sum(r["gemini_analysis"]["analysis_quality"] for r in results) / total_images
    avg_color_consistency = sum(r["color_accuracy"]["color_consistency"] for r in results) / total_images
    
    # Count schema versions
    schema_versions = {}
    for r in results:
        version = r["schema_version"]
        schema_versions[version] = schema_versions.get(version, 0) + 1
    
    summary = {
        "total_images": total_images,
        "passed_images": passed_images,
        "qa_pass_rate": qa_pass_rate,
        "avg_lpips": avg_lpips,
        "avg_clip_similarity": avg_clip_similarity,
        "avg_analysis_quality": avg_analysis_quality,
        "avg_color_consistency": avg_color_consistency,
        "schema_versions": schema_versions,
        "phase2_metrics": {
            "part_detection_success": sum(1 for r in results if r["part_count_match"]) / total_images,
            "gemini_analysis_success": sum(1 for r in results if r["gemini_analysis"]["has_analysis"]) / total_images,
            "color_accuracy_success": sum(1 for r in results if r["color_accuracy"]["color_consistency"] > 0.8) / total_images
        }
    }
    
    return {"results": results, "summary": summary}


def main():
    """Command line interface for Phase 2 batch QA"""
    parser = argparse.ArgumentParser(description="Run Phase 2 batch QA validation")
    parser.add_argument("--input", required=True, help="Input directory containing generated images")
    parser.add_argument("--facts", required=True, help="Directory containing facts JSON files")
    parser.add_argument("--report", required=True, help="Output QA report JSON file")
    parser.add_argument("--verbose", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    print("ğŸ” Phase 2 Batch QA Validation")
    print(f"ğŸ“ Input directory: {args.input}")
    print(f"ğŸ“ Facts directory: {args.facts}")
    print(f"ğŸ“„ Report file: {args.report}")
    
    try:
        qa_results = run_phase2_qa(args.input, args.facts)
        
        if "error" in qa_results:
            print(f"âŒ Error: {qa_results['error']}")
            return 1
        
        # Save results
        with open(args.report, 'w') as f:
            json.dump(qa_results, f, indent=2)
        
        # Print summary
        summary = qa_results["summary"]
        print(f"\nğŸ“Š QA Summary:")
        print(f"  Total images: {summary['total_images']}")
        print(f"  Passed images: {summary['passed_images']}")
        print(f"  QA pass rate: {summary['qa_pass_rate']:.1%}")
        print(f"  Avg LPIPS: {summary['avg_lpips']:.3f}")
        print(f"  Avg CLIP similarity: {summary['avg_clip_similarity']:.3f}")
        print(f"  Avg analysis quality: {summary['avg_analysis_quality']:.1%}")
        print(f"  Avg color consistency: {summary['avg_color_consistency']:.1%}")
        
        print(f"\nğŸ¯ Phase 2 Metrics:")
        phase2_metrics = summary["phase2_metrics"]
        print(f"  Part detection success: {phase2_metrics['part_detection_success']:.1%}")
        print(f"  Gemini analysis success: {phase2_metrics['gemini_analysis_success']:.1%}")
        print(f"  Color accuracy success: {phase2_metrics['color_accuracy_success']:.1%}")
        
        print(f"\nğŸ“‹ Schema versions:")
        for version, count in summary["schema_versions"].items():
            print(f"  {version}: {count} images")
        
        print(f"\nâœ… QA report saved to {args.report}")
        
        # Check if we meet Phase 2 targets
        targets_met = (
            summary['qa_pass_rate'] >= 0.90 and
            summary['avg_lpips'] <= 0.2 and
            summary['avg_clip_similarity'] >= 0.75 and
            phase2_metrics['part_detection_success'] >= 0.95 and
            phase2_metrics['gemini_analysis_success'] >= 0.90
        )
        
        if targets_met:
            print("ğŸ‰ Phase 2 targets met!")
        else:
            print("âš ï¸  Phase 2 targets not fully met - review results")
        
        return 0 if targets_met else 1
        
    except Exception as e:
        print(f"âŒ Error during QA validation: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
